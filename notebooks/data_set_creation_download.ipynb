{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download/Creation of Data Sets\n",
    "In this notebook, we download/create the necessary data sets for conducting the experiments presented in the accompanied article. The filepath where the data sets are stored is defined by the constant `evaluation.data_utils.DATA_PATH`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from evaluation.data_utils import DATA_PATH\n",
    "\n",
    "from skactiveml.utils import ExtLabelEncoder\n",
    "\n",
    "from sklearn.datasets import make_blobs, fetch_openml\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.datasets import CIFAR10, SVHN, FashionMNIST\n",
    "\n",
    "# Set random state to ensure reproducibility.\n",
    "RANDOM_STATE = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Toy Data Sets\n",
    "The following data set is generated for illustration purposes. It is a two-dimensional binary classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Generate toy data set for classification.\n",
    "data_set_name = \"toy-classification\"\n",
    "X, y_true = make_blobs(n_samples=500, centers=4, cluster_std=0.6, random_state=RANDOM_STATE)\n",
    "y_true %= 2\n",
    "y_true = y_true\n",
    "np.save(f\"{DATA_PATH}/{data_set_name}-X\", X.astype(np.float32))\n",
    "np.save(f\"{DATA_PATH}/{data_set_name}-y-true\", y_true.astype(np.int64))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_true)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download OpenML Data Sets\n",
    "In the following, we download standard data sets from the [OpenML](https://www.openml.org/search?type=data) repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "open_ml_data_sets = {\n",
    "    \"letter\": (6, np.float32, np.int64),\n",
    "    \"fmnist\": (None, np.float32, np.int64),\n",
    "    \"emnist\": (None, np.float32, np.int64),\n",
    "    \"cifar10\": (None, np.float32, np.int64),\n",
    "    \"svhn\": (None, np.float32, np.int64),\n",
    "}\n",
    "for data_set_name, (data_id, X_type, y_true_type) in open_ml_data_sets.items():\n",
    "    print(data_set_name)\n",
    "    sample_path = f\"{DATA_PATH}/{data_set_name}-X.npy\"\n",
    "    label_path = f\"{DATA_PATH}/{data_set_name}-y-true.npy\"\n",
    "    if os.path.isfile(sample_path) and os.path.isfile(label_path):\n",
    "        continue\n",
    "\n",
    "    # Download data.\n",
    "    if data_id:\n",
    "        X, y_true = fetch_openml(data_id=data_id, return_X_y=True)\n",
    "    else:\n",
    "        X = []\n",
    "        y_true = []\n",
    "        if data_set_name == \"fmnist\":\n",
    "            train_set = FashionMNIST(root=DATA_PATH, train=True, download=True, transform=ToTensor())\n",
    "            test_set = FashionMNIST(root=DATA_PATH, train=False, download=True, transform=ToTensor())\n",
    "        elif data_set_name == \"cifar10\":\n",
    "            train_set = CIFAR10(root=DATA_PATH, train=True, download=True, transform=ToTensor())\n",
    "            test_set = CIFAR10(root=DATA_PATH, train=False, download=True, transform=ToTensor())\n",
    "        elif data_set_name == \"svhn\":\n",
    "            train_set = SVHN(root=DATA_PATH, split=\"train\", download=True, transform=ToTensor())\n",
    "            test_set = SVHN(root=DATA_PATH, split=\"test\", download=True, transform=ToTensor())\n",
    "        loader = DataLoader(ConcatDataset([train_set, test_set]), batch_size=256, shuffle=False, num_workers=1)\n",
    "        for x, y in loader:\n",
    "            X.extend(x.numpy())\n",
    "            y_true.extend(y.numpy())\n",
    "        X = np.array(X)\n",
    "        print(X.sum())\n",
    "        y_true = np.array(y_true)\n",
    "\n",
    "    # Preprocess `X`.\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        X = X.values\n",
    "    X = X.astype(X_type)\n",
    "    if data_set_name in [\"fmnist\", \"emnist\"]:\n",
    "        X = X.reshape(len(X), 1, 28, 28)\n",
    "    elif data_set_name in [\"cifar10\", \"svhn\"]:\n",
    "        X = X.reshape(len(X), 3, 32, 32)\n",
    "\n",
    "    # Preprocess `y_true`.\n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.values\n",
    "    y_true = LabelEncoder().fit_transform(y_true)\n",
    "    y_true = y_true.astype(np.int64)\n",
    "\n",
    "    # Save data.\n",
    "    np.save(sample_path, X)\n",
    "    np.save(label_path, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Me Data Set\n",
    "Download the archive [Label Me](http://fprodrigues.com/deep_LabelMe.tar.gz) and extract its content as `LabelMe` directory to `DATA_PATH`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "data_set_name = \"label-me\"\n",
    "label_me_path = f\"{DATA_PATH}/LabelMe\"\n",
    "data_dict = {}\n",
    "\n",
    "# Load train, test, and validation data.\n",
    "for data_type in [\"train\", \"test\", \"valid\"]:\n",
    "    data_dict[f\"X-{data_type}\"] = (\n",
    "        np.load(f\"{label_me_path}/prepared/data_{data_type}_vgg16.npy\").astype(np.float32).reshape(-1, 8192)\n",
    "    )\n",
    "    data_dict[f\"y-true-{data_type}\"] = np.load(f\"{label_me_path}/prepared/labels_{data_type}.npy\").astype(np.int64)\n",
    "\n",
    "# Rename train data.\n",
    "data_dict[\"X\"] = data_dict.pop(\"X-train\")\n",
    "data_dict[\"y-true\"] = data_dict.pop(\"y-true-train\")\n",
    "data_dict[\"y\"] = np.load(f\"{label_me_path}/prepared/answers.npy\").astype(np.int64)\n",
    "\n",
    "# Compute annotator features.\n",
    "n_annotators = data_dict[\"y\"].shape[1]\n",
    "n_classes = len(np.unique(data_dict[\"y-true\"]))\n",
    "data_dict[\"A\"] = np.zeros((n_annotators, n_classes ** 2))\n",
    "for a_idx in range(n_annotators):\n",
    "    data_dict[\"A\"][a_idx] = confusion_matrix(\n",
    "        y_true=data_dict[\"y-true\"], y_pred=data_dict[\"y\"][:, a_idx], labels=np.arange(n_classes), normalize=\"all\"\n",
    "    ).ravel()\n",
    "\n",
    "# Save created numpy arrays.\n",
    "for key, item in data_dict.items():\n",
    "    np.save(f\"{DATA_PATH}/{data_set_name}-{key}.npy\", item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Music Genre Classification Data Set\n",
    "Download the archive [Music Genre Classification](http://fprodrigues.com//mturk-datasets.tar.gz) and extract its content as `music_genre_classification` directory to `DATA_PATH`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "data_set_name = \"music\"\n",
    "music_path = f\"{DATA_PATH}/music_genre_classification\"\n",
    "data_dict = {}\n",
    "\n",
    "# Load train and test data.\n",
    "train_df = pd.read_csv(f\"{music_path}/music_genre_gold.csv\", header=0)\n",
    "train_ids = train_df[\"id\"].values\n",
    "test_df = pd.read_csv(f\"{music_path}/music_genre_test.csv\", header=0)\n",
    "train_df_answers = pd.read_csv(f\"{music_path}/music_genre_mturk.csv\", header=0)\n",
    "\n",
    "# Setup label encoder and standard scaler.\n",
    "le = ExtLabelEncoder(classes=train_df[\"class\"].unique().astype(str), missing_label=\"not-available\")\n",
    "\n",
    "# Separate validation data.\n",
    "val_indices = np.random.RandomState(0).choice(np.arange(len(test_df)), replace=False, size=50)\n",
    "is_val = np.zeros(len(test_df), dtype=bool)\n",
    "is_val[val_indices] = True\n",
    "\n",
    "# Store train data.\n",
    "data_dict[\"y-true\"] = le.fit_transform(train_df[\"class\"].values.astype(str)).astype(np.int64)\n",
    "data_dict[\"X\"] = train_df.values[:, 1:-1].astype(np.float32)\n",
    "\n",
    "# Store validation data.\n",
    "data_dict[\"y-true-valid\"] = le.fit_transform(test_df[\"class\"].values[is_val].astype(str)).astype(np.int64)\n",
    "data_dict[\"X-valid\"] = test_df.values[is_val][:, 1:-1].astype(np.float32)\n",
    "\n",
    "# Store test data.\n",
    "data_dict[\"y-true-test\"] = le.fit_transform(test_df[\"class\"].values[~is_val].astype(str)).astype(np.int64)\n",
    "data_dict[\"X-test\"] = test_df.values[~is_val][:, 1:-1].astype(np.float32)\n",
    "\n",
    "# Store answers.\n",
    "annotators = train_df_answers[\"annotator\"].unique()\n",
    "n_annotators = len(annotators)\n",
    "data_dict[\"y\"] = np.full((len(train_df), n_annotators), fill_value=\"not-available\").astype(str)\n",
    "for row_idx, row in train_df_answers.iterrows():\n",
    "    sample_idx = np.where(train_ids == row[\"id\"])[0][0]\n",
    "    annotator_idx = np.where(annotators == row[\"annotator\"])[0][0]\n",
    "    data_dict[\"y\"][sample_idx, annotator_idx] = row[\"class\"]\n",
    "data_dict[\"y\"] = le.fit_transform(data_dict[\"y\"]).astype(np.int64)\n",
    "\n",
    "# Compute annotator features.\n",
    "n_annotators = data_dict[\"y\"].shape[1]\n",
    "n_classes = len(np.unique(data_dict[\"y-true\"]))\n",
    "data_dict[\"A\"] = np.zeros((n_annotators, n_classes ** 2))\n",
    "for a_idx in range(n_annotators):\n",
    "    data_dict[\"A\"][a_idx] = confusion_matrix(\n",
    "        y_true=data_dict[\"y-true\"], y_pred=data_dict[\"y\"][:, a_idx], labels=np.arange(n_classes), normalize=\"all\"\n",
    "    ).ravel()\n",
    "\n",
    "# Save created numpy arrays.\n",
    "for key, item in data_dict.items():\n",
    "    np.save(f\"{DATA_PATH}/{data_set_name}-{key}.npy\", item)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
