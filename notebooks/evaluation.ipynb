{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results of Experimental Evaluation\n",
    "In this notebook, we load the experimental results and summarize them in tabular form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from evaluation.config_utils import config_query, gen_results_table\n",
    "from evaluation.run_experiment import RESULT_PATH\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "plt.rc( 'text', usetex=True)\n",
    "\n",
    "# Flag indicating whether the results should be printed as LaTeX code.\n",
    "PRINT_LATEX = True"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Default Configurations of Multi-annotator Supervised Learning Techniques\n",
    "The following dictionary defines the default hyperparameters/settings of each multi-annotator supervised learning technique for the simulated annotator set independent and for the two data sets with real-world annotators. These configuration can be interpreted as kind of queries asking for certain results. One can alter the different parameters to ablate the effect of these parameters. Of course, in this case, the parameters need also to be adapted in the corresponding experiments."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_params = {\n",
    "    \"GT\": {\n",
    "        \"model_name\": \"gt\",\n",
    "        \"embed_x\": \"learned\",\n",
    "        \"confusion_matrix\": \"full\",\n",
    "        \"embed_size\": 16,\n",
    "        \"ap_use_residual\": True,\n",
    "        \"ap_use_outer_product\": True,\n",
    "        \"use_annotator_features\": False,\n",
    "    },\n",
    "    \"MR\": {\n",
    "        \"model_name\": \"mr\",\n",
    "        \"embed_x\": \"learned\",\n",
    "        \"confusion_matrix\": \"full\",\n",
    "        \"embed_size\": 16,\n",
    "        \"ap_use_residual\": True,\n",
    "        \"ap_use_outer_product\": True,\n",
    "        \"use_annotator_features\": False,\n",
    "    },\n",
    "    \"CL\": {\n",
    "        \"model_name\": \"cl\",\n",
    "    },\n",
    "    \"REAC\": {\n",
    "        \"model_name\": \"reac\",\n",
    "        \"lmbda\": 0.01,\n",
    "    },\n",
    "    \"UNION\": {\n",
    "        \"model_name\": \"union\",\n",
    "        \"epsilon\": 1e-5,\n",
    "    },\n",
    "    \"LIA\": {\n",
    "        \"model_name\": \"lia\",\n",
    "        \"ap_latent_dim\": 16,\n",
    "        \"n_em_steps\": 7,\n",
    "        \"n_fine_tune_epochs\": 25,\n",
    "        \"warm_start\": True,\n",
    "        \"use_annotator_features\": False,\n",
    "    },\n",
    "    \"CoNAL\": {\n",
    "        \"model_name\": \"conal\",\n",
    "        \"embed_size\": 20,\n",
    "        \"lmbda\": 1e-5,\n",
    "        \"use_annotator_features\": False,\n",
    "    },\n",
    "    \"MaDL(not X, I)\": {\n",
    "        \"model_name\": \"madl\",\n",
    "        \"embed_x\": \"none\",\n",
    "        \"confusion_matrix\": \"isotropic\",\n",
    "        \"embed_size\": 16,\n",
    "        \"eta\": 0.8,\n",
    "        \"alpha\": 1.25,\n",
    "        \"beta\": 0.25,\n",
    "        \"ap_use_residual\": False,\n",
    "        \"ap_use_outer_product\": False,\n",
    "        \"use_annotator_features\": False,\n",
    "    },\n",
    "    \"MaDL(not X, P)\": {\n",
    "        \"model_name\": \"madl\",\n",
    "        \"embed_x\": \"none\",\n",
    "        \"confusion_matrix\": \"diagonal\",\n",
    "        \"embed_size\": 16,\n",
    "        \"eta\": 0.8,\n",
    "        \"alpha\": 1.25,\n",
    "        \"beta\": 0.25,\n",
    "        \"ap_use_residual\": False,\n",
    "        \"ap_use_outer_product\": False,\n",
    "        \"use_annotator_features\": False,\n",
    "    },\n",
    "    \"MaDL(not X, F)\": {\n",
    "        \"model_name\": \"madl\",\n",
    "        \"embed_x\": \"none\",\n",
    "        \"confusion_matrix\": \"full\",\n",
    "        \"embed_size\": 16,\n",
    "        \"eta\": 0.8,\n",
    "        \"alpha\": 1.25,\n",
    "        \"beta\": 0.25,\n",
    "        \"ap_use_residual\": False,\n",
    "        \"ap_use_outer_product\": False,\n",
    "        \"use_annotator_features\": False,\n",
    "    },\n",
    "    \"MaDL(X, I)\": {\n",
    "        \"model_name\": \"madl\",\n",
    "        \"embed_x\": \"learned\",\n",
    "        \"confusion_matrix\": \"isotropic\",\n",
    "        \"embed_size\": 16,\n",
    "        \"eta\": 0.8,\n",
    "        \"alpha\": 1.25,\n",
    "        \"beta\": 0.25,\n",
    "        \"ap_use_residual\": True,\n",
    "        \"ap_use_outer_product\": True,\n",
    "        \"use_annotator_features\": False,\n",
    "    },\n",
    "    \"MaDL(X, P)\": {\n",
    "        \"model_name\": \"madl\",\n",
    "        \"embed_x\": \"learned\",\n",
    "        \"confusion_matrix\": \"diagonal\",\n",
    "        \"embed_size\": 16,\n",
    "        \"lmbda\": 0,\n",
    "        \"eta\": 0.8,\n",
    "        \"alpha\": 1.25,\n",
    "        \"beta\": 0.25,\n",
    "        \"ap_use_residual\": True,\n",
    "        \"ap_use_outer_product\": True,\n",
    "        \"use_annotator_features\": False,\n",
    "    },\n",
    "    \"MaDL(X, F)\": {\n",
    "        \"model_name\": \"madl\",\n",
    "        \"embed_x\": \"learned\",\n",
    "        \"confusion_matrix\": \"full\",\n",
    "        \"embed_size\": 16,\n",
    "        \"eta\": 0.8,\n",
    "        \"alpha\": 1.25,\n",
    "        \"beta\": 0.25,\n",
    "        \"ap_use_residual\": True,\n",
    "        \"ap_use_outer_product\": True,\n",
    "        \"use_annotator_features\": False,\n",
    "    },\n",
    "}\n",
    "data_type_dict = {\n",
    "    \"none\": \"independent\",\n",
    "    \"correlated\": \"interdependent\",\n",
    "    \"rand-dep_10_100\": \"random-interdependent\",\n",
    "    \"inductive_25\": \"inductive\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results for Default Parameters\n",
    "The following code cells produce the tables presenting the results with the default hyperparameters for different data and annotator sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set_list = [\"letter\", \"fmnist\", \"cifar10\", \"svhn\", \"music\", \"label-me\"]\n",
    "pm_symbol = \"$\\\\pm$\" if PRINT_LATEX else \"+-\"\n",
    "for data_type, annotator_set in data_type_dict.items():\n",
    "    params_dict = deepcopy(default_params)\n",
    "    if data_type in [\"correlated\", \"rand_dep_10_100\"]:\n",
    "        params_dict[\"MaDL(W)\"] = params_dict.pop(\"MaDL(X, F)\")\n",
    "        params_dict[\"MaDL(not W)\"] = deepcopy(params_dict[\"MaDL(W)\"])\n",
    "        params_dict[\"MaDL(not W)\"][\"alpha\"] = None\n",
    "        params_dict[\"MaDL(not W)\"][\"beta\"] = None\n",
    "    elif data_type == \"inductive_25\":\n",
    "        params_dict[\"GT\"][\"use_annotator_features\"] = True\n",
    "        params_dict[\"MR\"][\"use_annotator_features\"] = True\n",
    "        params_dict[\"LIA(not A)\"] = params_dict.pop(\"LIA\")\n",
    "        params_dict[\"LIA(A)\"] = deepcopy(params_dict[\"LIA(not A)\"])\n",
    "        params_dict[\"LIA(A)\"][\"use_annotator_features\"] = True\n",
    "        params_dict[\"CoNAL(not A)\"] = params_dict.pop(\"CoNAL\")\n",
    "        params_dict[\"CoNAL(A)\"] = deepcopy(params_dict[\"CoNAL(not A)\"])\n",
    "        params_dict[\"CoNAL(A)\"][\"use_annotator_features\"] = True\n",
    "        params_dict[\"MaDL(not A)\"] = params_dict.pop(\"MaDL(X, F)\")\n",
    "        params_dict[\"MaDL(A)\"] = deepcopy(params_dict[\"MaDL(not A)\"])\n",
    "        params_dict[\"MaDL(A)\"][\"use_annotator_features\"] = True\n",
    "    for data_set in data_set_list:\n",
    "        res_dict = {}\n",
    "        for model, model_dict in params_dict.items():\n",
    "            config_dict = {\n",
    "                \"data_set_name\": data_set,\n",
    "                \"data_type\": data_type,\n",
    "                **model_dict,\n",
    "            }\n",
    "            try:\n",
    "                model_res_dict = config_query(\n",
    "                    path=RESULT_PATH,\n",
    "                    config_dict=config_dict,\n",
    "                )\n",
    "            except Exception as e:\n",
    "                continue\n",
    "            if len(model_res_dict) > 0:\n",
    "                res_dict.setdefault(\"model\", []).append(model)\n",
    "                best_run = None\n",
    "                best_val_acc = None\n",
    "                for run, res in model_res_dict.items():\n",
    "                    val_acc = np.array(res[\"valid-micro-accuracy-gt\"])\n",
    "                    if best_val_acc is None:\n",
    "                        best_val_acc = val_acc\n",
    "                        best_run = np.array([run] * len(val_acc), dtype=object)\n",
    "                    is_better = val_acc > best_val_acc\n",
    "                    best_val_acc[is_better] = val_acc[is_better]\n",
    "                    best_run[is_better] = run\n",
    "                for idx, run in enumerate(best_run):\n",
    "                    for perf, scores in model_res_dict[run].items():\n",
    "                        if \"test\" in perf:\n",
    "                            res_dict.setdefault(perf, [])\n",
    "                            if idx == 0:\n",
    "                                res_dict[perf].append([])\n",
    "                            res_dict[perf][-1].append(scores[idx])\n",
    "                            if idx == len(best_run) - 1:\n",
    "                                res_dict[perf][-1] = np.array(res_dict[perf][-1])\n",
    "                                res_dict[perf][\n",
    "                                    -1\n",
    "                                ] = f\"{np.round(res_dict[perf][-1].mean(), 3):.3f} {pm_symbol} {np.round(res_dict[perf][-1].std(), 3):.3f}\"\n",
    "        if len(res_dict) > 0:\n",
    "            print(f\"Data Set: {data_set}; Annotator Set: {annotator_set}\")\n",
    "            res_df = pd.DataFrame(res_dict)\n",
    "            appendix = \"\" if \"inductive\" in data_type else \"\"\n",
    "            columns = [\n",
    "                \"model\",\n",
    "                \"test-micro-accuracy-gt\",\n",
    "                \"test-cross-entropy-gt\",\n",
    "                \"test-brier-score-gt\",\n",
    "                \"test-micro-accuracy-ap\" + appendix,\n",
    "                \"test-cross-entropy-ap\" + appendix,\n",
    "                \"test-brier-score-ap\" + appendix,\n",
    "                \"test-macro-accuracy-ap\" + appendix,\n",
    "            ]\n",
    "            res_df = res_df[columns]\n",
    "            if PRINT_LATEX:\n",
    "                latex = res_df.to_latex()\n",
    "                latex = latex.replace(\"\\\\$\", \"$\").replace(\"\\\\textbackslash pm\", \"\\\\pm\").replace(\"nan$\\\\pm$nan\", \"--\").replace(\" $\\\\pm$ \", \"$\\\\pm$\")\n",
    "                print(latex)\n",
    "            else:\n",
    "                display(HTML(res_df.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results for One-factor-at-a-time Ablation Study\n",
    "The following code cells produce tables presenting the results of the ablation study for MaDL. Thereby, each table shows the results for one of pair of data and annotator set. Since it is a one-factor-at-a-time study, only the parameter named in the corresponding is changed from the default configuration of MaDL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hyper_param_dict = {\n",
    "    \"MaDL(X, F)\": [\"embed_size\", \"eta\", \"ap_use_outer_product\", \"ap_use_residual\", \"alpha\"],\n",
    "}\n",
    "data_set_list = [\"letter\", \"music\", \"label-me\"]\n",
    "for data_type, annotator_set in data_type_dict.items():\n",
    "    for data_set in data_set_list:\n",
    "        appendix = \"-inductive\" if \"inductive\" in data_type else \"\"\n",
    "        columns = [\n",
    "            \"model\",\n",
    "            \"param\",\n",
    "            \"value\",\n",
    "            \"test-micro-accuracy-gt\",\n",
    "            \"test-cross-entropy-gt\",\n",
    "            \"test-brier-score-gt\",\n",
    "            \"test-micro-accuracy-ap\",\n",
    "            \"test-cross-entropy-ap\",\n",
    "            \"test-brier-score-ap\",\n",
    "            \"test-macro-accuracy-ap\",\n",
    "        ]\n",
    "        res_df = pd.DataFrame(columns=columns)\n",
    "        for model, hyper_param_list in hyper_param_dict.items():\n",
    "            for hyper_param in hyper_param_list:\n",
    "                res_dict = {}\n",
    "                model_dict = deepcopy(default_params[model])\n",
    "                if data_type == \"inductive_25\":\n",
    "                    model_dict[\"use_annotator_features\"] = True\n",
    "                config_dict = {\n",
    "                    \"data_set_name\": data_set,\n",
    "                    \"data_type\": data_type,\n",
    "                    **model_dict,\n",
    "                }\n",
    "                if hyper_param == \"alpha\":\n",
    "                    config_dict.pop(\"alpha\")\n",
    "                    config_dict.pop(\"beta\")\n",
    "                else:\n",
    "                    config_dict.pop(hyper_param)\n",
    "\n",
    "                try:\n",
    "                    model_res_dict = config_query(\n",
    "                        path=RESULT_PATH,\n",
    "                        config_dict=config_dict,\n",
    "                    )\n",
    "                except Exception:\n",
    "                    continue\n",
    "\n",
    "                if len(model_res_dict) > 0:\n",
    "                    model_res_df = gen_results_table(model_res_dict, param=hyper_param, decimals=7)\n",
    "                    model_res_df[\"model\"] = model\n",
    "                    model_res_df = model_res_df[columns]\n",
    "                    res_df = res_df.append(model_res_df)\n",
    "        if len(res_df) > 0:\n",
    "            print(f\"Data Set: {data_set}; Annotator Set: {annotator_set}\")\n",
    "            if PRINT_LATEX:\n",
    "                latex = res_df.to_latex()\n",
    "                latex = latex.replace(\" +- \", \"$\\\\pm$\").replace(\"nan$\\\\pm$nan\", \"--\")\n",
    "                print(latex)\n",
    "            else:\n",
    "                display(HTML(res_df.to_html(index=False)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###  Results for Varying Annotation Ratios on CIFAR10\n",
    "The following code cells produce the results for the CIFAR10 dataset and varying annotation ratios, i.e., $\\{0.2, 0.4, 0.6, 0.8\\}$."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pm_symbol = \"$\\\\pm$\" if PRINT_LATEX else \"+-\"\n",
    "params_dict = deepcopy(default_params)\n",
    "mlr_array = np.array([0.8, 0.6, 0.4, 0.2])\n",
    "data_type = \"none\"\n",
    "data_set = \"cifar100\"\n",
    "res_df_list = []\n",
    "res_df_list_std = []\n",
    "columns = [\n",
    "    \"model\",\n",
    "    \"test-micro-accuracy-gt\",\n",
    "    \"test-cross-entropy-gt\",\n",
    "    \"test-brier-score-gt\",\n",
    "    \"test-micro-accuracy-ap\",\n",
    "    \"test-cross-entropy-ap\",\n",
    "    \"test-brier-score-ap\",\n",
    "    \"test-macro-accuracy-ap\",\n",
    "]\n",
    "for mlr in mlr_array:\n",
    "    res_dict = {}\n",
    "    res_dict_std = {}\n",
    "    for model, model_dict in params_dict.items():\n",
    "        if model in [\"MaDL(not X, I)\", \"MaDL(not X, P)\", \"MaDL(X, I)\", \"MaDL(X, P)\", \"MaDL(not X, F)\"]:\n",
    "            continue\n",
    "        config_dict = {\n",
    "            \"data_set_name\": data_set,\n",
    "            \"data_type\": data_type,\n",
    "            \"missing_label_ratio\": mlr,\n",
    "            **model_dict,\n",
    "        }\n",
    "        try:\n",
    "            model_res_dict = config_query(\n",
    "                path=RESULT_PATH,\n",
    "                config_dict=config_dict,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            continue\n",
    "        if len(model_res_dict) > 0:\n",
    "            res_dict.setdefault(\"model\", []).append(model)\n",
    "            res_dict_std.setdefault(\"model\", []).append(model)\n",
    "            best_run = None\n",
    "            best_val_acc = None\n",
    "            for run, res in model_res_dict.items():\n",
    "                val_acc = np.array(res[\"valid-micro-accuracy-gt\"])\n",
    "                if best_val_acc is None:\n",
    "                    best_val_acc = val_acc\n",
    "                    best_run = np.array([run] * len(val_acc), dtype=object)\n",
    "                is_better = val_acc > best_val_acc\n",
    "                best_val_acc[is_better] = val_acc[is_better]\n",
    "                best_run[is_better] = run\n",
    "            for idx, run in enumerate(best_run):\n",
    "                for perf, scores in model_res_dict[run].items():\n",
    "                    if \"test\" in perf:\n",
    "                        res_dict.setdefault(perf, [])\n",
    "                        res_dict_std.setdefault(perf, [])\n",
    "                        if idx == 0:\n",
    "                            res_dict[perf].append([])\n",
    "                            res_dict_std[perf].append([])\n",
    "                        res_dict[perf][-1].append(scores[idx])\n",
    "                        if idx == len(best_run) - 1:\n",
    "                            res_dict[perf][-1] = np.array(res_dict[perf][-1])\n",
    "                            res_dict_std[perf][-1] = np.round(res_dict[perf][-1].std(), 3)\n",
    "                            res_dict[perf][-1] = np.round(res_dict[perf][-1].mean(), 3)\n",
    "    if len(res_dict) > 0:\n",
    "        print(f\"Data Set: {data_set}; Annotator Set: independent\")\n",
    "        res_df = pd.DataFrame(res_dict)\n",
    "        res_df_std = pd.DataFrame(res_dict_std)\n",
    "        res_df = res_df[columns]\n",
    "        res_df_std = res_df_std[columns]\n",
    "        res_df.set_index(\"model\", inplace=True)\n",
    "        res_df_std.set_index(\"model\", inplace=True)\n",
    "        res_df_list.append(res_df)\n",
    "        res_df_list_std.append(res_df_std)\n",
    "        if PRINT_LATEX:\n",
    "            latex = res_df.to_latex()\n",
    "            latex = latex.replace(\"\\\\$\", \"$\").replace(\"\\\\textbackslash pm\", \"\\\\pm\").replace(\"nan$\\\\pm$nan\", \"--\")\n",
    "            print(latex)\n",
    "        else:\n",
    "            display(HTML(res_df.to_html()))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cm = plt.get_cmap('rainbow')\n",
    "annotation_ratios = 1 - mlr_array\n",
    "color_dict = {\n",
    "    \"GT\": [\"k\", \"-\"],\n",
    "    \"MR\": [\"k\", \"--\"],\n",
    "    \"LIA\": [\"r\", \"-\"],\n",
    "    \"REAC\": [\"g\", \"-\"],\n",
    "    \"CL\": [\"orange\", \"-\"],\n",
    "    \"UNION\": [\"b\", \"-\"],\n",
    "    \"CoNAL\": [\"y\", \"-\"],\n",
    "    \"MaDL(not X, F)\": [\"b\", \"--\"],\n",
    "    \"MaDL(X, F)\": [\"b\", \"-\"],\n",
    "}\n",
    "print(len(res))\n",
    "for col in columns[1:]:\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_prop_cycle(color=[cm(1.*i/len(res_df_list[0])) for i in range(len(res_df_list[0]))])\n",
    "    perfs = pd.concat([res[col].T for res in res_df_list], axis=1)\n",
    "    perfs_std = pd.concat([res[col].T for res in res_df_list_std], axis=1)\n",
    "    for i in perfs.index:\n",
    "        plt.errorbar(x=annotation_ratios, y=perfs.loc[i].values.T, yerr=perfs_std.loc[i].values.T, label=i)\n",
    "    ax.set_xticks(annotation_ratios)\n",
    "    ax.yaxis.set_major_locator(plt.MaxNLocator(5))\n",
    "    ax.yaxis.set_major_formatter(FormatStrFormatter('$%.2f$'))\n",
    "    ax.tick_params(axis='both', which='major', labelsize=16)\n",
    "    ax.tick_params(axis='both', which='minor', labelsize=16)\n",
    "    plt.savefig(f\"../figures/{data_set}-annotation_ratios-{col}.pdf\", bbox_inches=\"tight\", pad_inches=0)\n",
    "    plt.tight_layout()\n",
    "    plt.legend()\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from evaluation.data_utils import load_data, DATA_PATH\n",
    "from evaluation.experiment_utils import aggregate_labels\n",
    "from lfma.utils import introduce_missing_annotations\n",
    "\n",
    "# Load data set.\n",
    "seed = 0\n",
    "data_type = \"none\"\n",
    "data_set = \"cifar10\"\n",
    "ds = load_data(data_path=DATA_PATH, data_set_name=data_set, data_type=data_type, n_repeats=5, valid_size=0.05, test_size=0.2)\n",
    "\n",
    "# Missing label ratios to be tested.\n",
    "mlr_array = np.array([0.8, 0.6, 0.4, 0.2])\n",
    "annotation_ratios = 1 - mlr_array\n",
    "\n",
    "# Compute accuracy of the majority vote and fraction of correction annotations.\n",
    "mr_accuracies = []\n",
    "correct_annotation_proportion = []\n",
    "for missing_label_ratio in mlr_array:\n",
    "    mr_accuracies.append([])\n",
    "    correct_annotation_proportion.append([])\n",
    "    n_iter = 0\n",
    "    for tr, val, te in zip(ds[\"train\"], ds[\"valid\"], ds[\"test\"]):\n",
    "        n_iter += 1\n",
    "\n",
    "        # Get true labels of training samples.\n",
    "        y_true_train = ds[\"y_true\"][tr]\n",
    "\n",
    "        # Randomly add missing labels.\n",
    "        y_partial = introduce_missing_annotations(\n",
    "            y=ds[\"y\"][tr],\n",
    "            missing_label=-1,\n",
    "            percentage=missing_label_ratio,\n",
    "            random_state=seed + n_iter,\n",
    "        )\n",
    "        y_mr = aggregate_labels(y=y_partial, y_true=y_true_train, aggregation_method=\"mr\")\n",
    "        is_lbld = y_mr != -1\n",
    "        accuracy = np.mean(y_mr[is_lbld] == y_true_train[is_lbld])\n",
    "        mr_accuracies[-1].append(accuracy)\n",
    "        is_lbld = y_partial != -1\n",
    "        proportion = np.sum((y_partial == y_true_train[:, None]) * is_lbld)/np.sum(is_lbld)\n",
    "        correct_annotation_proportion[-1].append(proportion)\n",
    "\n",
    "# Plot results.\n",
    "fig = plt.figure(figsize=(6.4, 4.9))\n",
    "ax = fig.add_subplot(111)\n",
    "plt.errorbar(x=1-mlr_array, y=np.mean(mr_accuracies, axis=1), yerr=np.std(mr_accuracies, axis=1), label=\"majority rule\", c=\"k\")\n",
    "plt.errorbar(x=1-mlr_array, y=np.mean(correct_annotation_proportion, axis=1), yerr=np.std(correct_annotation_proportion, axis=1), label=\"correct annotation proportion\", c=\"k\", ls=\":\")\n",
    "ax.set_xticks(annotation_ratios)\n",
    "ax.yaxis.set_major_locator(plt.MaxNLocator(5))\n",
    "ax.yaxis.set_major_formatter(FormatStrFormatter('$%.2f$'))\n",
    "ax.tick_params(axis='both', which='major', labelsize=16)\n",
    "ax.tick_params(axis='both', which='minor', labelsize=16)\n",
    "plt.savefig(f\"../figures/{data_set}-annotation-accuracies.pdf\", bbox_inches=\"tight\", pad_inches=0)\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Results for Varying Numbers of Annotated Instance on LETTER\n",
    "\n",
    "The following code cells produce the results for the LETTER dataset for varying number of instances."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pm_symbol = \"$\\\\pm$\" if PRINT_LATEX else \"+-\"\n",
    "params_dict = deepcopy(default_params)\n",
    "tr_array = np.array([512, 1024, 1536, 2048, 2560, 3072, 3584, 4096, 4608, 5120])\n",
    "data_type = \"none\"\n",
    "data_set = \"letter\"\n",
    "res_df_list = []\n",
    "res_df_list_std = []\n",
    "columns = [\n",
    "    \"model\",\n",
    "    \"test-micro-accuracy-gt\",\n",
    "    \"test-cross-entropy-gt\",\n",
    "    \"test-brier-score-gt\",\n",
    "    \"test-micro-accuracy-ap\",\n",
    "    \"test-cross-entropy-ap\",\n",
    "    \"test-brier-score-ap\",\n",
    "    \"test-macro-accuracy-ap\",\n",
    "]\n",
    "for tr in tr_array:\n",
    "    res_dict = {}\n",
    "    res_dict_std = {}\n",
    "    for model, model_dict in params_dict.items():\n",
    "        config_dict = {\n",
    "            \"data_set_name\": data_set,\n",
    "            \"data_type\": data_type,\n",
    "            \"training_size\": tr,\n",
    "            **model_dict,\n",
    "        }\n",
    "        try:\n",
    "            model_res_dict = config_query(\n",
    "                path=RESULT_PATH,\n",
    "                config_dict=config_dict,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            continue\n",
    "        if len(model_res_dict) > 0:\n",
    "            res_dict.setdefault(\"model\", []).append(model)\n",
    "            res_dict_std.setdefault(\"model\", []).append(model)\n",
    "            best_run = None\n",
    "            best_val_acc = None\n",
    "            for run, res in model_res_dict.items():\n",
    "                val_acc = np.array(res[\"valid-micro-accuracy-gt\"])\n",
    "                if best_val_acc is None:\n",
    "                    best_val_acc = val_acc\n",
    "                    best_run = np.array([run] * len(val_acc), dtype=object)\n",
    "                is_better = val_acc > best_val_acc\n",
    "                best_val_acc[is_better] = val_acc[is_better]\n",
    "                best_run[is_better] = run\n",
    "            for idx, run in enumerate(best_run):\n",
    "                for perf, scores in model_res_dict[run].items():\n",
    "                    if \"test\" in perf:\n",
    "                        res_dict.setdefault(perf, [])\n",
    "                        res_dict_std.setdefault(perf, [])\n",
    "                        if idx == 0:\n",
    "                            res_dict[perf].append([])\n",
    "                            res_dict_std[perf].append([])\n",
    "                        res_dict[perf][-1].append(scores[idx])\n",
    "                        if idx == len(best_run) - 1:\n",
    "                            res_dict[perf][-1] = np.array(res_dict[perf][-1])\n",
    "                            res_dict_std[perf][-1] = np.round(res_dict[perf][-1].std(), 3)\n",
    "                            res_dict[perf][-1] = np.round(res_dict[perf][-1].mean(), 3)\n",
    "    if len(res_dict) > 0:\n",
    "        print(f\"Data Set: {data_set}; Annotator Set: independent\")\n",
    "        res_df = pd.DataFrame(res_dict)\n",
    "        res_df_std = pd.DataFrame(res_dict_std)\n",
    "        res_df = res_df[columns]\n",
    "        res_df_std = res_df_std[columns]\n",
    "        res_df.set_index(\"model\", inplace=True)\n",
    "        res_df_std.set_index(\"model\", inplace=True)\n",
    "        res_df_list.append(res_df)\n",
    "        res_df_list_std.append(res_df_std)\n",
    "        if PRINT_LATEX:\n",
    "            latex = res_df.to_latex()\n",
    "            latex = latex.replace(\"\\\\$\", \"$\").replace(\"\\\\textbackslash pm\", \"\\\\pm\").replace(\"nan$\\\\pm$nan\", \"--\")\n",
    "            print(latex)\n",
    "        else:\n",
    "            display(HTML(res_df.to_html()))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cm = plt.get_cmap('rainbow')\n",
    "color_dict = {\n",
    "    \"GT\": [\"k\", \"-\"],\n",
    "    \"MR\": [\"k\", \"--\"],\n",
    "    \"LIA\": [\"r\", \"-\"],\n",
    "    \"REAC\": [\"g\", \"-\"],\n",
    "    \"CL\": [\"orange\", \"-\"],\n",
    "    \"UNION\": [\"b\", \"-\"],\n",
    "    \"CoNAL\": [\"y\", \"-\"],\n",
    "    \"MaDL(not X, F)\": [\"b\", \"--\"],\n",
    "    \"MaDL(X, F)\": [\"b\", \"-\"],\n",
    "}\n",
    "col = \"test-micro-accuracy-ap\"\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_prop_cycle(color=[cm(1.*i/len(res_df_list[0])) for i in range(len(res_df_list[0]))])\n",
    "perfs = pd.concat([res[col].T for res in res_df_list], axis=1)\n",
    "perfs_std = pd.concat([res[col].T for res in res_df_list_std], axis=1)\n",
    "for i in perfs.index:\n",
    "    plt.errorbar(x=tr_array, y=perfs.loc[i].values.T, yerr=perfs_std.loc[i].values.T, label=i)\n",
    "ax.set_xticks(tr_array)\n",
    "ax.set_yticks(np.array([0.5, 0.6, 0.7]))\n",
    "ax.tick_params(axis='both', which='major', labelsize=16)\n",
    "ax.tick_params(axis='both', which='minor', labelsize=16)\n",
    "plt.savefig(f\"../figures/{data_set}-training_size-{col}.pdf\", bbox_inches=\"tight\", pad_inches=0)\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
